{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EV Car Prices\n",
    "\n",
    "This assignment focuses on car prices. The data ('car_prices.xlsx') is a pre-processed version of original data scraped from bilbasen.dk by previous MAL1 students. The dataset contains 16 columns:\n",
    "\n",
    "- **Price (DKK)**: The current listed price of the vehicle in Danish Kroner.\n",
    "- **Model Year**: The manufacturing year of the vehicle.\n",
    "- **Mileage (km)**: The total kilometres driven by the vehicle (odometer reading).\n",
    "- **Electric Range (km)**: The estimated maximum driving range on a full charge.\n",
    "- **Battery Capacity (kWh)**: The total capacity of the vehicle's battery in kilowatt-hours.\n",
    "- **Energy Consumption (Wh/km)**: The vehicle's energy consumption in watt-hours per kilometre.\n",
    "- **Annual Road Tax (DKK)**: The annual road tax cost in Danish Kroner.\n",
    "- **Horsepower (bhp)**: The vehicle's horsepower (brake horsepower).\n",
    "- **0-100 km/h (s)**: The time (in seconds) for the car to accelerate from 0 to 100 km/h.\n",
    "- **Top Speed (km/h)**: The maximum speed the vehicle can achieve.\n",
    "- **Towing Capacity (kg)**: The maximum weight the vehicle can tow.\n",
    "- **Original Price (DKK)**: The price of the vehicle when first sold as new.\n",
    "- **Number of Doors**: The total number of doors on the vehicle.\n",
    "- **Rear-Wheel Drive**: A binary indicator (1 = Yes, 0 = No) for rear-wheel drive.\n",
    "- **All-Wheel Drive (AWD)**: A binary indicator (1 = Yes, 0 = No) for all-wheel drive.\n",
    "- **Front-Wheel Drive**: A binary indicator (1 = Yes, 0 = No) for front-wheel drive.\n",
    "\n",
    "The first one, **Price**, is the response variable.\n",
    "\n",
    "The **objective** of this assignment is:\n",
    "1. Understand how linear algebra is used in Machine Learning, specifically for correlations and regression\n",
    "2. Learn how to perform multiple linear regression, ridge regression, lasso regression and elastic net\n",
    "3. Learn how to assess regression models\n",
    "\n",
    "Please solve the tasks using this notebook as you template, i.e. insert code blocks and markdown block to this notebook and hand it in. Please use 42 as your random seed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    " - Import the dataset \n",
    " - Split the data in a training set and test set - make sure you extract the response variable\n",
    " - Remember to use the data appropriately; in the tasks below, we do not explicitly state when to use train and test - but in order to compare the models, you must use the same dataset for training and testing in all models.\n",
    " - Output: When you are done with this, you should have the following sets: `X` (the original dataset), `X_train`, `X_train`, `X_test`, `y_train`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('/Users/bruger/Desktop/car_prices.xlsx')\n",
    "\n",
    "# Display basic info and first few rows\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "# Define features and response variable\n",
    "y = df['Price (DKK)']\n",
    "X = df.drop(columns=['Price (DKK)'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train multiple linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse_lin = mean_squared_error(y_test, y_pred_lin)\n",
    "r2_lin = r2_score(y_test, y_pred_lin)\n",
    "print(f'Linear Regression MSE: {mse_lin}')\n",
    "print(f'Linear Regression R^2 Score: {r2_lin}')\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "print(f'Ridge Regression MSE: {mean_squared_error(y_test, y_pred_ridge)}')\n",
    "print(f'Ridge Regression R^2 Score: {r2_score(y_test, y_pred_ridge)}')\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "print(f'Lasso Regression MSE: {mean_squared_error(y_test, y_pred_lasso)}')\n",
    "print(f'Lasso Regression R^2 Score: {r2_score(y_test, y_pred_lasso)}')\n",
    "\n",
    "# Elastic Net Regression\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "y_pred_elastic = elastic_net.predict(X_test)\n",
    "print(f'Elastic Net MSE: {mean_squared_error(y_test, y_pred_elastic)}')\n",
    "print(f'Elastic Net R^2 Score: {r2_score(y_test, y_pred_elastic)}')\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred_lin, label='Linear Regression', alpha=0.6)\n",
    "sns.scatterplot(x=y_test, y=y_pred_ridge, label='Ridge Regression', alpha=0.6)\n",
    "sns.scatterplot(x=y_test, y=y_pred_lasso, label='Lasso Regression', alpha=0.6)\n",
    "sns.scatterplot(x=y_test, y=y_pred_elastic, label='Elastic Net', alpha=0.6)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Prices')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Algebra\n",
    "In this assignment, you have to solve all problems using linear algebra concepts. You are free to use SymPy or NumPy - though NumPy is **significantly** more efficient computationally than SymPy since NumPy is optimized for numerical computations with floating-point arithmetic. Since linear regression is purely numerical, NumPy is the better choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression finds the best-fitting line (or hyperplane) by solving for the **coefficient vector** $\\mathbf{B}$ that minimizes the squared error:\n",
    "\n",
    "$$\n",
    "\\mathbf{B} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X}$ is the **design matrix**, including a column of ones for the intercept.\n",
    "- $\\mathbf{y}$ is the **response variable** (target values).\n",
    "- $\\mathbf{B}$ contains the **regression coefficients**.\n",
    "\n",
    "**Explanation of Each Step**\n",
    "1. **Construct the matrix $X$**:\n",
    "   - Each **row** represents a data point.\n",
    "   - Each **column** represents a feature.\n",
    "   - The **first column is all ones** to account for the **intercept**.\n",
    "\n",
    "2. **Solve for $\\mathbf{B}$ using the normal equation**:\n",
    "   - Compute $X^T X$ (feature correlation).\n",
    "   - Compute $X^T y$ (cross-product with the target variable).\n",
    "   - Compute the **inverse of $X^T X$** and multiply by $X^T y$ to get $\\mathbf{B}$.\n",
    "\n",
    "3. **Interpret the results**:\n",
    "   - The **first value** in $\\mathbf{B}$ is the **intercept**.\n",
    "   - The remaining values are the **coefficients for each feature**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example \n",
    "X = np.array([[1, 1.1, 2.2], \n",
    "              [1, 2.3, 3.1], \n",
    "              [1, 3.3, 4.2], \n",
    "              [1, 4.0, 5.0]])  # The first column is for the intercept\n",
    "\n",
    "y = np.array([2.5, 3.8, 5.1, 6.2])  # Target variable\n",
    "\n",
    "# Compute the normal equation: B = (X^T X)^(-1) X^T y\n",
    "XtX = np.dot(X.T, X)\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "XtY = np.dot(X.T, y)\n",
    "B = np.dot(XtX_inv, XtY)\n",
    "\n",
    "# Display the regression coefficients\n",
    "B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the regression coefficients $\\mathbf{B}$, we can evaluate how well the model fits the data using two key metrics:\n",
    "\n",
    "1. **Mean Squared Error (MSE)** – Measures the average squared difference between the predicted and actual values:\n",
    "   $$\n",
    "   MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "   $$\n",
    "   - Lower MSE means better fit.\n",
    "\n",
    "2. **$R^2$ (Coefficient of Determination)** – Measures how much of the variance in $y$ is explained by $X$:\n",
    "   $$\n",
    "   R^2 = 1 - \\frac{\\sum (y - \\hat{y})^2}{\\sum (y - \\bar{y})^2}\n",
    "   $$\n",
    "   - $R^2$ ranges from **0 to 1**, where **1** indicates a perfect fit and **0** means the model explains no variance.\n",
    "\n",
    "\n",
    "**Explanation of Each Step**\n",
    "1. **Compute Predictions**:  \n",
    "   $$ \\hat{y} = X B $$\n",
    "   This gives the model’s predicted values.\n",
    "\n",
    "2. **Compute MSE**:  \n",
    "   - We square the residuals $ (y - \\hat{y})^2 $ and take the mean.\n",
    "\n",
    "3. **Compute $R^2$**:\n",
    "   - **Total sum of squares** $ SS_{total} $ measures the total variance in $ y $.\n",
    "   - **Residual sum of squares** $ SS_{residual} $ measures the variance left unexplained by the model.\n",
    "   - $ R^2 $ tells us what fraction of variance is explained.\n",
    "\n",
    "**Interpreting the Results**\n",
    "- **MSE**: Lower values indicate a better fit.\n",
    "- **$R^2$ Score**:\n",
    "  - **$R^2 = 1$** → Perfect fit (all points on the regression line).\n",
    "  - **$R^2 = 0$** → Model is no better than predicting the mean of $ y $.\n",
    "  - **$R^2 < 0$** → Model performs worse than a simple average.\n",
    "\n",
    "Implement the above steps using linear algebra so that you both create a regression model and calculate the MSE and $R^2$. Note, here you need to use `X_train`, `X_test`, `y_train` and `y_test` appropriately!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for Task 2. Add more code blocks if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Using Library Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Correlation and OLS\n",
    "For this task you must do the following\n",
    " - Using library functions, build the following models:\n",
    "   - Correlation matrix where the correlations are printed in the matrix and a heat map is overlaid\n",
    "   - Ordinary least squares\n",
    "   - Performance metrics: MSE, RMSE, $R^2$\n",
    "   - Comment on the real world meaning of RMSE and $R^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for Task 4. Add more code blocks if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Ridge, Lasso and Elastic Net\n",
    "In order for Ridge and Lasso (and Elastic net) to have an effect, you must use scaled data to build the models, since regularization depends on coefficient magnitude, and if using non-scaled data the penalty will affect them unequally. Feel free to use this code to scale the data:\n",
    "\n",
    "```python\n",
    "# Standardize X\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Standardize y\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "```\n",
    "For the final task you must do the following\n",
    "   - Ridge regression (using multiple alphas)\n",
    "   - Lasso regression (using multiple alphas)\n",
    "   - Elastic Net (using multiple alphas)\n",
    " - Discussion and conclusion:\n",
    "   - Discuss the MSE and $R^2$ of all 3 models and conclude which model has the best performance - note the MSE will be scaled!\n",
    "   - Rebuild the OLS model from Task 4, but this time use the scaled data from this task - interpret the meaning of the model's coefficients\n",
    "   - Use the coefficients of the best ridge and lasso model to print the 5 most important features and compare to the 5 most important features in the OLS with scaled data model. Do the models agree about which features are the most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You may get a convergence warning; try increasing the `max_iter` parameter of the model (the default is 1000 - maybe set it to 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for Task 5. Add more code blocks if needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
